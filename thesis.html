<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Scalable Inference on High-Dimensional Earth Observation
Datacubes</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<div class="page-wrapper">
<div class="title-page-content">
<div class="title-header">
<h1 style="font-size: 22pt; margin-bottom: 0.5em;">
Scalable Inference on High-Dimensional Earth Observation Datacubes
</h1>
</div>
<div class="title-degree">
<h3 style="font-weight: normal; margin-bottom: 0.5em;">
Bachelor Thesis
</h3>
<p style="text-align: center;">
Submitted in partial fulfillment of the requirements for the degree of
</p>
<h3 style="margin-top: 0.5em;">
Bachelor of Science in Computer Science
</h3>
</div>
<div class="title-university">
<p style="text-align: center; font-weight: bold; font-size: 14pt;">
Constructor University
</p>
<p style="text-align: center;">
School of Computer Science and Engineering
</p>
</div>
<div class="title-date">
<p style="text-align: center;">
<strong>Date:</strong> December 15, 2025
</p>
</div>
<div class="title-footer">
<div class="author-block">
<p>
<strong>Author:</strong> Rati Vrdiashvili
</p>
<p>
<strong>Supervisor:</strong> Prof. Dr. Peter Baumann
</p>
</div>
</div>
</div>
</div>
<div class="page-wrapper">
<section id="statutory-declaration" style="margin-top: 30%;">
<h1>Statutory Declaration</h1>
<p><br></p>
<p>I, <strong>Rati Vrdiashvili</strong>, hereby declare that I have
written this Bachelor Thesis independently, unless where clearly stated
otherwise. I have used only the sources, the data and the support that I
have clearly mentioned. This Bachelor Thesis has not been submitted for
conferral of a degree elsewhere.</p>
<p><br><br><br><br></p>
<hr />
<p>Signature</p>
<p><br></p>
<p><strong>Date:</strong> December 15, 2025</p>
<p><strong>Place:</strong> Bremen, Germany</p>
</section>
</div>
<section id="table-of-contents" class="page-wrapper content-page">
<h1>Table of Contents</h1>
<ul class="toc-list">
<li>
<a href="#chapter-1-introduction">1. Introduction</a>
<ul>
<li>
<a href="#context-the-era-of-big-earth-data-and-ai">1.1 Context: The Era
of Big Earth Data and AI</a>
</li>
<li>
<a href="#problem-statement-the-logical-vs.-physical-mismatch">1.2
Problem Statement</a>
</li>
<li>
<a href="#research-objectives">1.3 Research Objectives</a>
</li>
<li>
<a href="#thesis-structure">1.4 Thesis Structure</a>
</li>
<li>
<a href="#code-availability">1.5 Code Availability</a>
</li>
</ul>
</li>
<li>
<a href="#chapter-2-theoretical-background">2. Theoretical
Background</a>
<ul>
<li>
<a href="#earth-observation-data-structures">2.1 Earth Observation Data
Structures</a>
</li>
<li>
<a href="#deep-learning-in-remote-sensing">2.2 Deep Learning in Remote
Sensing</a>
</li>
<li>
<a href="#related-work-and-state-of-the-art">2.3 Related Work and State
of the Art</a>
</li>
</ul>
</li>
<li>
<a href="#chapter-3-methodology---the-geospatial-inference-pipeline-gsip">3.
Methodology (GSIP)</a>
<ul>
<li>
<a href="#system-design-philosophy">3.1 System Design Philosophy</a>
</li>
<li>
<a href="#the-memory-model-zone-of-responsibility">3.2 The Memory Model:
Zone of Responsibility</a>
</li>
<li>
<a href="#producer-consumer-architecture">3.3 Producer-Consumer
Architecture</a>
</li>
<li>
<a href="#multi-modal-fusion-strategy">3.4 Multi-Modal Fusion
Strategy</a>
</li>
</ul>
</li>
<li>
<a href="#chapter-4-algorithmic-solution---seamless-reconstruction">4.
Algorithmic Solution</a>
<ul>
<li>
<a href="#the-overlap-tile-strategy">4.1 The Overlap-Tile Strategy</a>
</li>
<li>
<a href="#probabilistic-aggregation-sinusoidal-weighting">4.2
Probabilistic Aggregation</a>
</li>
<li>
<a href="#uncertainty-quantification">4.3 Uncertainty Quantification</a>
</li>
</ul>
</li>
<li>
<a href="#chapter-5-evaluation">5. Evaluation</a>
<ul>
<li>
<a href="#experimental-setup">5.1 Experimental Setup</a>
</li>
<li>
<a href="#quantitative-performance">5.2 Quantitative Performance</a>
</li>
<li>
<a href="#qualitative-analysis">5.3 Qualitative Analysis</a>
</li>
<li>
<a href="#failure-analysis">5.4 Failure Analysis</a>
</li>
<li>
<a href="#operational-cost-analysis">5.5 Operational Cost Analysis</a>
</li>
</ul>
</li>
<li>
<a href="#chapter-6-discussion">6. Discussion</a>
</li>
<li>
<a href="#chapter-7-conclusion">7. Conclusion</a>
</li>
<li>
<a href="#references">8. References</a>
</li>
<li>
<a href="#appendix-a-usage-guide">Appendix A: Usage Guide</a>
</li>
</ul>
</section>
<div class="page-wrapper content-page">
<h1 id="chapter-1-introduction">Chapter 1: Introduction</h1>
<h2 id="context-the-era-of-big-earth-data-and-ai">1.1 Context: The Era
of Big Earth Data and AI</h2>
<p>The field of Earth Observation (EO) is currently undergoing a
paradigm shift driven by the exponential growth of open satellite data.
The European Space Agency’s (ESA) Copernicus programme, specifically the
Sentinel constellation, generates approximately 10-12 terabytes of
high-resolution imagery daily. Sentinel-2 alone, with its 10-meter
spatial resolution and 5-day revisit time, provides a continuously
updating digital twin of the Earth’s surface. This data deluge has
transformed Remote Sensing from a discipline of manual interpretation
and sparse data analysis into a domain of “Big Data,” where the limiting
factor is no longer data acquisition but data processing.</p>
<p>Concurrently, the rise of Deep Learning (DL), particularly
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has
revolutionized computer vision. The transition from “feature
engineering” (e.g., Random Forests on manually calculated indices like
NDVI) to “feature learning” has enabled unprecedented accuracy in
complex tasks. Most recently, the emergence of <strong>Foundation
Models</strong>—large-scale pre-trained backbones such as the IBM/NASA
Prithvi model—promises to generalize across diverse geographies and
seasons, offering “off-the-shelf” intelligence for semantic
segmentation, land cover classification, and change detection. However,
a significant engineering gap remains between the availability of these
sophisticated models and their operational deployment on the massive
scale of satellite archives.</p>
<p>The traditional workflow of downloading images to a local
workstation, processing them with a GUI-based tool (like QGIS or SNAP),
and uploading the results is fundamentally unscalable in the petabyte
era. We are witnessing a transition towards “Cloud-Native” processing,
where code moves to data. Yet, the actual <em>inference engines</em>—the
software components responsible for executing the neural networks—are
often ad-hoc scripts that lack the robustness required for production
environments.</p>
<h2 id="problem-statement-the-logical-vs.-physical-mismatch">1.2 Problem
Statement: The Logical vs. Physical Mismatch</h2>
<p>The central challenge addressed in this thesis is the fundamental
mismatch between the logical view of EO data and the physical
constraints of modern computing hardware.</p>
<h3 id="the-logical-view-continuous-fields">1.2.1 The Logical View:
Continuous Fields</h3>
<p>Logically, a scientist views a satellite product (e.g., a Sentinel-2
tile or a country-wide mosaic) as a single, continuous function <span
class="math inline">\(f(x, y, t)\)</span> over a spatial domain <span
class="math inline">\(\Omega\)</span>. They seek to apply a learned
operator—the neural network—to this domain to derive semantic insights
(e.g., <span class="math inline">\(g(x) = \text{flood\_risk}\)</span>).
The domain is theoretically infinite; a river does not stop at the edge
of a JPEG file.</p>
<h3 id="the-physical-reality-discrete-tensors">1.2.2 The Physical
Reality: Discrete Tensors</h3>
<p>Physically, however, these domains are massive multi-dimensional
arrays, or “datacubes,” often exceeding tens of gigabytes in size for a
single scene. A standard Sentinel-2 L1C granule is <span
class="math inline">\(10,980 \times 10,980\)</span> pixels with 13
spectral bands. Storing this as a Float32 tensor requires approximately:
<span class="math display">\[ 10980^2 \times 13 \times 4 \text{ bytes}
\approx 6.3 \text{ GB} \]</span> This is just for the input.
Intermediate activation maps in a deep U-Net can easily inflate this
memory requirement by a factor of 10 or 20 during the forward pass.</p>
<p>Modern GPU accelerators, while powerful, are bound by relatively
scarce Video Random Access Memory (VRAM), typically ranging from 16GB
(Consumer) to 80GB (Data Center). This necessitates the partitioning of
the domain into smaller, manageable sub-units.</p>
<h3 id="the-cv-bias-and-grid-artifacts">1.2.3 The “CV Bias” and Grid
Artifacts</h3>
<p>The standard approach in Computer Vision—resizing images to fixed
dimensions like <span class="math inline">\(224 \times 224\)</span>
(ImageNet standard)—is inapplicable in EO. In a photograph of a cat,
resizing reduces the file size while preserving the semantic content
(the cat). In satellite imagery, pixel resolution corresponds to
physical ground sampling distance (GSD). Resizing a 10m/pixel image to
fit a neural network effectively destroys the small-scale features
(roads, buildings, streams) that the model is supposed to detect.</p>
<p>Consequently, “tiling” or “patching” becomes mandatory. Yet, naive
tiling introduces severe discontinuities at patch boundaries, known as
“grid artifacts.” These arise because Convolutional Neural Networks lose
spatial context at the edges of their input (the Zero-Padding problem).
When these independent tiles are stitched back together, the result is a
checkerboard pattern of discontinuities. These artifacts render the
resulting probability maps scientifically invalid for downstream
applications such as hydrological modeling, where a continuous river
network is essential.</p>
<h2 id="research-objectives">1.3 Research Objectives</h2>
<p>This thesis proposes the <strong>GeoSpatial Inference Pipeline
(GSIP)</strong>, a high-performance computing framework designed to
bridge the gap between deep learning research and operational earth
observation. The primary research objectives are:</p>
<ol type="1">
<li><strong>Formalization of Patch-Based Inference:</strong> To define a
mathematically rigorous framework for the decomposition and
reconstruction of tensor fields that guarantees <span
class="math inline">\(C^1\)</span> continuity and eliminates edge
effects through probabilistic soft-voting mechanisms (Sinusoidal
Weighting).</li>
<li><strong>Scalable and Agnostic Architecture:</strong> To design a
modular system architecture that decouples the underlying tiling logic
from the specific machine learning model. Utilizing an <strong>Adapter
Pattern</strong>, the system must support diverse architectures—from
legacy ResNets to modern Vision Transformers (Prithvi)—without
modification to the core engine.</li>
<li><strong>Memory-Aware Computing:</strong> To develop a dynamic
resource management algorithm, the <strong>Zone of Responsibility
(ZoR)</strong>, which estimates optimal data chunk sizes based on
real-time hardware profiling. This ensures the system can process
gigapixel-scale images on consumer-grade hardware without Out-Of-Memory
(OOM) crashes, democratizing access to high-end EO analysis.</li>
</ol>
<h2 id="thesis-structure">1.4 Thesis Structure</h2>
<p>The remainder of this thesis is organized as follows: *
<strong>Chapter 2</strong> provides the theoretical background on
Datacubes, the OGC standards, and the specific challenges of the
“Effective Receptive Field” in CNNs. It also reviews related work in
tiling strategies. * <strong>Chapter 3</strong> details the system
architecture of GSIP, focusing on the Producer-Consumer multiprocessing
model and the Zone of Responsibility algorithm. * <strong>Chapter
4</strong> presents the core algorithmic contribution: the mathematical
formulation of the seamless reconstruction strategy using 2D window
functions. * <strong>Chapter 5</strong> evaluates the system’s
performance, offering quantitative benchmarks on throughput and memory
stability. * <strong>Chapter 6</strong> discusses the implications for
integrating such inference engines into Array Databases like Rasdaman as
User Defined Functions (UDFs). * <strong>Chapter 7</strong> concludes
with a summary of contributions and outlines future work in OGC API
integration.</p>
<h2 id="code-availability">1.5 Code Availability</h2>
<p>The core source code for the GeoSpatial Inference Pipeline (GSIP),
including the adapter implementations and memory management logic, is
open-source and available at: <a
href="https://github.com/rvardiashvili/GSIP"><strong>https://github.com/rvardiashvili/GSIP</strong></a></p>
<h1 id="chapter-2-theoretical-background">Chapter 2: Theoretical
Background</h1>
<h2 id="earth-observation-data-structures">2.1 Earth Observation Data
Structures</h2>
<p>To understand the engineering challenges of EO inference, one must
first define the data model. In the context of High-Performance
Computing and Array Databases, Earth Observation data is best
represented as a <strong>Datacube</strong>.</p>
<h3 id="the-multi-dimensional-array">2.1.1 The Multi-Dimensional
Array</h3>
<p>Formally, a Datacube <span class="math inline">\(D\)</span> is a
dense, multi-dimensional array defined as: <span class="math display">\[
D \in \mathbb{R}^{X \times Y \times T \times B} \]</span> where: * <span
class="math inline">\(X, Y\)</span> represent the spatial dimensions
(latitude/longitude or projected Easting/Northing). * <span
class="math inline">\(T\)</span> represents the temporal dimension
(acquisition time). * <span class="math inline">\(B\)</span> represents
the spectral bands (e.g., Blue, Green, Red, NIR, SWIR).</p>
<p>Unlike standard RGB images used in Computer Vision (<span
class="math inline">\(X \times Y \times 3\)</span>), EO data is
inherently geospatial. Every pixel <span class="math inline">\((i,
j)\)</span> corresponds to a coordinate on the Earth’s surface via an
Affine Transform matrix and a Coordinate Reference System (CRS).</p>
<h3 id="the-alignment-problem-in-multi-modal-fusion">2.1.2 The Alignment
Problem in Multi-Modal Fusion</h3>
<p>A critical challenge in modern EO is <strong>Multi-Modal
Fusion</strong>—combining Optical (e.g., Sentinel-2) and Radar (e.g.,
Sentinel-1) data. These sensors have fundamentally different acquisition
geometries. * <strong>Sentinel-2</strong> is an optical push-broom
sensor, typically delivered in the UTM (Universal Transverse Mercator)
projection, divided into <span class="math inline">\(100km \times
100km\)</span> tiles (MGRS grid). * <strong>Sentinel-1</strong> is a
Synthetic Aperture Radar (SAR) side-looking sensor. Its Level-1 GRD
(Ground Range Detected) products are often delivered in a WGS84 CRS or a
satellite-specific geometry, with pixels that do not align one-to-one
with the Sentinel-2 grid.</p>
<p>Naive stacking of these arrays as simple NumPy tensors is
geometrically invalid. Precise, sub-pixel alignment is required to
ensure that a vector at index <span class="math inline">\((i,
j)\)</span> corresponds to the exact same physical location on the
Earth’s surface across all modalities. This necessitates
<strong>on-the-fly reprojection</strong> (warping), a computationally
expensive operation that typically requires cubic spline
interpolation.</p>
<h2 id="deep-learning-in-remote-sensing">2.2 Deep Learning in Remote
Sensing</h2>
<p>The state-of-the-art for extracting semantic information from these
datacubes lies in Deep Learning.</p>
<h3 id="cnns-and-effective-receptive-field-erf">2.2.1 CNNs and Effective
Receptive Field (ERF)</h3>
<p>Convolutional Neural Networks (CNNs) like U-Net or ResNet build a
representation of the input through successive layers of convolution and
pooling. A fundamental concept is the <strong>Receptive
Field</strong>—the region of input pixels that theoretically contributes
to a specific output pixel. While the <em>theoretical</em> receptive
field of a deep network might cover the entire input image, empirical
studies (Luo et al., 2016) show that the <strong>Effective Receptive
Field (ERF)</strong> follows a Gaussian distribution centered on the
output pixel. The gradient magnitude decays rapidly from the center to
the periphery.</p>
<p><strong>Implication for Tiling:</strong> This Gaussian distribution
means that predictions made at the spatial edges of an input patch are
inherently less reliable than those at the center. At the edge, the
kernel “sees” the zero-padding (artificial data) rather than the true
neighboring pixels. This is the theoretical root cause of grid
artifacts.</p>
<h3 id="foundation-models-the-case-of-prithvi">2.2.2 Foundation Models:
The Case of Prithvi</h3>
<p>Recently, the field has moved towards <strong>Foundation
Models</strong>. The <strong>Prithvi-100M</strong> model, developed by
IBM and NASA, represents this shift. It is a Vision Transformer (ViT)
based on the Masked Autoencoder (MAE) architecture, pre-trained on the
Harmonized Landsat Sentinel-2 (HLS) dataset. *
<strong>Architecture:</strong> Unlike CNNs which are
translation-invariant, ViTs split the image into rigid patches (e.g.,
<span class="math inline">\(16 \times 16\)</span> pixels) and process
them as a sequence of tokens. * <strong>Inference Challenge:</strong>
Prithvi typically expects a fixed input size (e.g., <span
class="math inline">\(224 \times 224 \times 6\)</span>). Running this
model on a <span class="math inline">\(10,000 \times 10,000\)</span>
pixel image requires not just tiling, but careful management of the
positional embeddings to ensure the model understands the spatial
context.</p>
<h3 id="state-of-the-art-vision-transformers-in-eo">2.2.3 State of the
Art: Vision Transformers in EO</h3>
<p>The dominance of CNNs is currently being challenged by <strong>Vision
Transformers (ViTs)</strong>. Originally designed for Natural Language
Processing (NLP), Transformers leverage the <strong>Self-Attention
Mechanism</strong> to model long-range dependencies, which is
particularly beneficial in Remote Sensing where context (e.g., a river
flowing from top-left to bottom-right) spans the entire image.</p>
<ul>
<li><strong>Swin Transformer (Liu et al., 2021):</strong> Introduced a
hierarchical transformer whose representation is computed with shifted
windows. This architecture is efficient for dense prediction tasks like
segmentation and has become a standard backbone in EO competitions.</li>
<li><strong>SatMAE (Cong et al., 2022):</strong> Applied Masked
Autoencoders (MAE) to satellite imagery. By masking 75% of the input
patches and forcing the model to reconstruct them, SatMAE learns robust
spectral-spatial representations without needing millions of labeled
labels.</li>
<li><strong>Prithvi (Jakubik et al., 2023):</strong> Built upon the MAE
foundation, Prithvi is specialized for HLS (Harmonized Landsat Sentinel)
data, incorporating temporal attention to handle time-series.</li>
</ul>
<p>This shift towards ViTs exacerbates the tiling problem. While CNNs
have a “fading” receptive field, ViTs have a global receptive field
(within the patch sequence). Cutting a ViT’s input field arbitrarily at
a tile boundary disrupts the positional embeddings, making the need for
overlap and smooth reconstruction even more critical.</p>
<h2 id="related-work-and-state-of-the-art">2.3 Related Work and State of
the Art</h2>
<p>The problem of tiling artifacts is well-known, and several solutions
exist, though few offer a complete production-grade pipeline.</p>
<h3 id="naive-tiling-the-baseline">2.3.1 Naive Tiling (The
Baseline)</h3>
<p>The simplest approach is non-overlapping sliding windows. This is the
default behavior of many academic scripts. * <strong>Method:</strong>
Split image into <span class="math inline">\(N\)</span> blocks. Infer
<span class="math inline">\(N\)</span> blocks. Stitch <span
class="math inline">\(N\)</span> blocks. * <strong>Drawback:</strong>
Guaranteed discontinuities at boundaries. <span
class="math inline">\(C^0\)</span> continuity is not preserved</p>
<h3 id="torchgeo-and-gridgeosampler">2.3.2 TorchGeo and
GridGeoSampler</h3>
<p><strong>TorchGeo</strong> is a PyTorch domain library that provides
specific data loaders for geospatial data. * <strong>Approach:</strong>
It offers a <code>GridGeoSampler</code> and
<code>RandomGeoSampler</code> which can perform sliding window
inference. * <strong>Limitation:</strong> While excellent for research
and training, TorchGeo focuses on the <em>Data Loading</em> step. It
does not strictly enforce a memory management model for the
<em>Inference</em> step (e.g., handling the VRAM accumulation of
logits). It leaves the “stitching” logic largely to the user or simple
averaging methods.</p>
<h3 id="solaris-cosmiq-works">2.3.3 Solaris (CosmiQ Works)</h3>
<p><strong>Solaris</strong> was an early attempt to build a pipeline for
vector-to-raster operations. * <strong>Approach:</strong> It included
tools for tiling and stitching. * <strong>Status:</strong> It is largely
unmaintained. Its tiling logic was often disk-based (writing thousands
of small TIFs), which creates a massive I/O bottleneck compared to
in-memory tensor slicing.</p>
<h3 id="array-databases-rasdaman-scidb">2.3.4 Array Databases (Rasdaman,
SciDB)</h3>
<p>In the database world, systems like <strong>Rasdaman</strong> handle
“tiling” natively at the storage level. * <strong>Approach:</strong>
Data is stored in pre-defined tiles (BLOBs) in the database. Queries are
executed via OGC WCPS (Web Coverage Processing Service). * <strong>The
Gap:</strong> Traditionally, these databases excelled at arithmetic
operations (<code>NDVI = (NIR-Red)/(NIR+Red)</code>) but struggled with
Deep Learning inference which requires loading complex Python libraries
(PyTorch/TensorFlow) and massive model weights into the database kernel.
GSIP aims to bridge this gap by acting as a potential external execution
engine. # Chapter 3: Methodology - The GeoSpatial Inference Pipeline
(GSIP)</p>
<h2 id="system-design-philosophy">3.1 System Design Philosophy</h2>
<p>The GeoSpatial Inference Pipeline (GSIP) is designed as a
high-throughput, fault-tolerant system for operating on raster data that
exceeds system memory. It adheres to the <strong>Principle of Separation
of Concerns</strong>: the <em>Physical Tiling</em> (how data is moved
from disk to RAM) is decoupled from the <em>Logical Inference</em> (what
the neural network actually does).</p>
<h3 id="the-adapter-pattern">3.1.1 The Adapter Pattern</h3>
<p>To ensure the system remains model-agnostic, GSIP employs the
<strong>Adapter Design Pattern</strong>. The core engine does not import
<code>torchvision.models.resnet</code> or
<code>transformers.Prithvi</code>. Instead, it interacts with a
<code>BaseAdapter</code> abstract interface.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseAdapter(ABC):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> preprocess(<span class="va">self</span>, chunk: np.ndarray) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Normalizes raw pixels to model distribution (e.g. z-score).&quot;&quot;&quot;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> postprocess(<span class="va">self</span>, logits: torch.Tensor) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Converts raw logits to probability maps or class indices.&quot;&quot;&quot;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> input_shape(<span class="va">self</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">int</span>, <span class="bu">int</span>]:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Returns required (Height, Width) e.g., (224, 224).&quot;&quot;&quot;</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div>
<p>This polymorphism allows the pipeline to act as a universal “driver.”
Whether the underlying model is a binary flood detector (1 output
channel) or a multi-spectral land cover classifier (19 output channels),
the pipeline logic remains identical.</p>
<h2 id="the-memory-model-zone-of-responsibility">3.2 The Memory Model:
Zone of Responsibility</h2>
<p>A critical innovation of GSIP is the <strong>Zone of Responsibility
(ZoR)</strong> algorithm. In standard deep learning scripts, the “batch
size” and “tile size” are often hardcoded hyperparameters. This works
for homogenous cluster environments but fails in heterogeneous
deployment (e.g., moving from a DGX station to a laptop).</p>
<p>GSIP inverts this dependency. It views the <em>Available RAM</em> as
the independent variable and the <em>Tile Size</em> as the dependent
variable.</p>
<h3 id="the-cost-function">3.2.1 The Cost Function</h3>
<p>We define the memory cost function for processing a single spatial
chunk of dimensions <span class="math inline">\(L \times L\)</span>
as:</p>
<p><span class="math display">\[ M_{total}(L) = M_{input}(L) +
M_{gpu}(L) + M_{logits}(L) + M_{recon}(L) + C_{overhead} \]</span></p>
<p>Where: * <span class="math inline">\(M_{input} \approx L^2 \cdot
B_{in} \cdot 4 \text{ bytes}\)</span> (Float32 Input Tensor) * <span
class="math inline">\(M_{logits} \approx L^2 \cdot C_{out} \cdot 4
\text{ bytes}\)</span> (Float32 Output Logits) * <span
class="math inline">\(M_{recon} \approx L^2 \cdot C_{out} \cdot 4 \text{
bytes}\)</span> (Accumulation Buffer)</p>
<p>For a segmentation task with <span
class="math inline">\(C_{out}=19\)</span> classes (BigEarthNet), the
output tensors dominate. The algorithm solves for the maximum <span
class="math inline">\(L\)</span> such that <span
class="math inline">\(M_{total}(L) &lt; \alpha \cdot
\text{RAM}_{available}\)</span>, where <span
class="math inline">\(\alpha \approx 0.8\)</span> is a safety
factor.</p>
<h3 id="dynamic-resolution">3.2.2 Dynamic Resolution</h3>
<p>At runtime, the <code>calculate_optimal_zor()</code> function probes
the OS (via <code>psutil</code>) to determine free memory. It
iteratively tests increasing values of <span
class="math inline">\(L\)</span> (e.g., 1024, 2048, 4096) until the cost
function approaches the safety limit. This ensures that the system
maximally utilizes available resources without triggering the OS
Out-Of-Memory (OOM) killer.</p>
<h2 id="producer-consumer-architecture">3.3 Producer-Consumer
Architecture</h2>
<p>Deep Learning inference on satellite imagery is a
<strong>Bound-Varying Problem</strong>: 1. <strong>I/O Bound:</strong>
Reading compressed GeoTIFFs from disk. 2. <strong>Compute
Bound:</strong> Running the ResNet/ViT forward pass on GPU. 3.
<strong>Memory Bound:</strong> Merging the massive output probability
maps.</p>
<p>A sequential loop (<code>Read -&gt; Infer -&gt; Write</code>) would
leave the GPU idle during Read/Write phases. GSIP utilizes a
<strong>Producer-Consumer</strong> parallel architecture implemented via
<code>torch.multiprocessing</code>.</p>
<h3 id="the-pipeline-flow">3.3.1 The Pipeline Flow</h3>
<ol type="1">
<li><strong>Main Process (The Producer):</strong>
<ul>
<li><strong>Action:</strong> Slices the large input image into the
calculated “Chunks” (ZoR).</li>
<li><strong>Action:</strong> Performs “Lazy Reprojection” of Sentinel-1
data (using <code>rasterio.vrt</code>).</li>
<li><strong>Action:</strong> Pushes normalized tensors into the
<code>Inference Queue</code>.</li>
<li><em>Optimization:</em> Uses a background <code>Prefetcher</code>
thread to ensure the Queue is never empty.</li>
</ul></li>
<li><strong>Inference Engine (GPU Worker):</strong>
<ul>
<li><strong>Action:</strong> Pulls batches from the queue.</li>
<li><strong>Action:</strong> Moves data to CUDA device
(asynchronously).</li>
<li><strong>Action:</strong> Executes <code>model(x)</code> in
<code>torch.no_grad()</code> mode.</li>
<li><strong>Action:</strong> Pushes raw logits (on CPU) to the
<code>Writer Queue</code>.</li>
</ul></li>
<li><strong>Writer Process (The Consumer):</strong>
<ul>
<li><strong>Action:</strong> Pulls logits.</li>
<li><strong>Action:</strong> Executes the <strong>Sinusoidal
Reconstruction</strong> (see Chapter 4).</li>
<li><strong>Action:</strong> Computes Uncertainty metrics
(Entropy).</li>
<li><strong>Action:</strong> Writes final GeoTIFFs to storage.</li>
</ul></li>
</ol>
<p>This architecture creates a “Pipelined” effect where the GPU is
consistently saturated, masking the latency of disk I/O and CPU
post-processing.</p>
<h2 id="multi-modal-fusion-strategy">3.4 Multi-Modal Fusion
Strategy</h2>
<p>The pipeline natively handles sensor fusion. The configuration allows
specifying multiple data sources. * <strong>Logical Alignment:</strong>
The system treats the Sentinel-2 grid as the “Anchor.” * <strong>Virtual
Warping:</strong> Sentinel-1 data is not physically reprojected on disk
(which would double storage requirements). Instead, GSIP constructs a
<code>WarperVRT</code> in memory. When a chunk is requested, GDAL
computes the spline interpolation on-the-fly to align the SAR pixels
with the Optical pixels. This “Virtual Data Cube” approach is essential
for handling petabyte-scale archives where data duplication is
prohibited. # Chapter 4: Algorithmic Solution - Seamless
Reconstruction</p>
<h2 id="the-overlap-tile-strategy">4.1 The Overlap-Tile Strategy</h2>
<p>The fundamental algorithmic contribution of this work is the rigorous
implementation of the <strong>Overlap-Tile Strategy</strong> to solve
the problem of Grid Artifacts.</p>
<h3 id="the-need-for-overlap">4.1.1 The Need for Overlap</h3>
<p>As established in Chapter 2, the Effective Receptive Field (ERF) of a
CNN decays towards the edges. If we tile an image with zero overlap,
pixels at the boundary <span class="math inline">\(\partial P_i\)</span>
are predicted using only partial information (padded zeros). This
results in predictions that are statistically distinct from those at the
center, creating visible seams.</p>
<p>We define the tiling parameters: * <span
class="math inline">\(P\)</span>: The Patch Size (Input to Model), e.g.,
<span class="math inline">\(224 \times 224\)</span> pixels. * <span
class="math inline">\(S\)</span>: The Stride (Step Size), e.g., <span
class="math inline">\(112\)</span> pixels. * <strong>Overlap
Ratio:</strong> <span class="math inline">\((P - S) / P = 0.5\)</span>
(50%).</p>
<p>This 50% overlap ensures that every pixel in the valid output domain
<span class="math inline">\(\Omega&#39;\)</span> is covered by exactly
<strong>four</strong> predictions (in the 2D case, barring image edges):
Top-Left, Top-Right, Bottom-Left, and Bottom-Right relative to the patch
centers.</p>
<h2 id="probabilistic-aggregation-sinusoidal-weighting">4.2
Probabilistic Aggregation: Sinusoidal Weighting</h2>
<p>Ideally, we want to discard the “weak” predictions made at the patch
edges and retain the “strong” predictions made at the patch centers. A
simple arithmetic mean <span class="math inline">\(\frac{1}{N}\sum
y_i\)</span> fails to do this; it dilutes the good center prediction
with the bad edge prediction. Linearly weighted blending (triangular
windows) is an improvement but suffers from discontinuities in the first
derivative at the peak, which can still be visible as subtle artifacts
in gradient-based downstream applications (e.g., edge detection).</p>
<p>We employ a <strong>Soft-Voting</strong> mechanism using a 2D
<strong>Hann Window</strong> (Squared Sine), which guarantees
higher-order continuity.</p>
<h3 id="the-2d-hann-window">4.2.1 The 2D Hann Window</h3>
<p>We define a 1D window function <span
class="math inline">\(w(t)\)</span> for a domain <span
class="math inline">\(t \in [0, P-1]\)</span>: <span
class="math display">\[ w(t) = \sin^2\left(\frac{\pi t}{P-1}\right)
\]</span> This function has desirable properties: * <span
class="math inline">\(w(0) = 0\)</span> and <span
class="math inline">\(w(P-1) = 0\)</span> (Zero weight at edges). *
<span class="math inline">\(w(P/2) = 1\)</span> (Maximum weight at
center). * Smooth, differentiable curve (<span
class="math inline">\(C^\infty\)</span> continuous).</p>
<p>The 2D Weight Mask <span class="math inline">\(W(x, y)\)</span> is
constructed as the outer product: <span class="math display">\[ W(x, y)
= w(x) \otimes w(y) = \sin^2\left(\frac{\pi x}{P-1}\right) \cdot
\sin^2\left(\frac{\pi y}{P-1}\right) \]</span></p>
<h3 id="the-reconstruction-formula">4.2.2 The Reconstruction
Formula</h3>
<p>Let <span class="math inline">\(O(u, v)\)</span> be the reconstructed
probability at global coordinate <span class="math inline">\((u,
v)\)</span>. Let <span class="math inline">\(Pred_k\)</span> be the
prediction tensor from the <span class="math inline">\(k\)</span>-th
patch, located at offset <span class="math inline">\((x_k,
y_k)\)</span>. Let <span class="math inline">\(W_k\)</span> be the
weight window for that patch.</p>
<p><span class="math display">\[ O(u, v) = \frac{\sum_{k} W(u-x_k,
v-y_k) \cdot Pred_k(u-x_k, v-y_k)}{\sum_{k} W(u-x_k, v-y_k)}
\]</span></p>
<h3 id="partition-of-unity-and-flux-conservation">4.2.3 Partition of
Unity and Flux Conservation</h3>
<p>A key mathematical property of the squared sine function is the
<strong>Partition of Unity</strong> when shifted by half a period (<span
class="math inline">\(\\pi/2\)</span> phase shift): <span
class="math display">\[ \sin^2(x) + \sin^2(x + \pi/2) = \sin^2(x) +
\cos^2(x) = 1 \]</span> When extended to the 2D stride of <span
class="math inline">\(P/2\)</span>, this ensures that the denominator
<span class="math inline">\(\sum W\)</span> is spatially constant (flat)
across the valid reconstruction zone. * <strong>Contrast with
Gaussian:</strong> Gaussian weights never reach exactly zero and do not
sum to a constant 1, requiring explicit normalization that can introduce
numerical instability or “brightness modulation” in the probability map.
* <strong>Contrast with Linear:</strong> Linear blending creates a
“pyramid” shape. The sum is constant, but the derivative is
discontinuous at the peaks.</p>
<p>GSIP’s use of Sinusoidal Weighting ensures that the output
probability map is not only continuous but also smooth, preventing any
“banding” artifacts in derived products.</p>
<h2 id="uncertainty-quantification">4.3 Uncertainty Quantification</h2>
<p>A distinct advantage of this probabilistic aggregation is that it
allows us to quantify <strong>Epistemic Uncertainty</strong>. Since we
hold an ensemble of <span class="math inline">\(N=4\)</span> predictions
for every pixel, we can measure the <em>disagreement</em> between these
predictions.</p>
<h3 id="shannon-entropy">4.3.1 Shannon Entropy</h3>
<p>We compute the pixel-wise Entropy <span
class="math inline">\(H\)</span> on the aggregated probability
distribution: <span class="math display">\[ H(x) = - \sum_{c=1}^{C}
p_c(x) \log_2 (p_c(x) + \epsilon) \]</span> *
<strong>Interpretation:</strong> * <strong>Low Entropy (<span
class="math inline">\(H \to 0\)</span>):</strong> The model is confident
(e.g., center of a deep lake). * <strong>High Entropy (<span
class="math inline">\(H \to \log_2(C)\)</span>):</strong> The model is
uniformly confused (e.g., cloud edges, mixed pixels).</p>
<h3 id="prediction-gap">4.3.2 Prediction Gap</h3>
<p><span class="math display">\[ Gap = P_{top1} - P_{top2} \]</span>
This metric measures the margin of safety. A small gap (e.g., 0.05)
indicates the model is “on the fence” between two classes (e.g.,
distinguishing “Forest” from “Dense Shrubland”).</p>
<p>These uncertainty maps are exported alongside the class predictions.
In a decision-support context (e.g., flood response), an analyst can use
the Entropy map to mask out unreliable areas, a capability not provided
by standard “Argmax” inference scripts. # Chapter 5: Evaluation</p>
<h2 id="experimental-setup">5.1 Experimental Setup</h2>
<p>To validate the efficiency and correctness of GSIP, we designed a
series of experiments benchmarking it against standard “Naive”
implementations.</p>
<h3 id="validated-hardware-specifications">5.1.1 Validated Hardware
Specifications</h3>
<p>The experiments were conducted on a consumer-grade laptop workstation
to demonstrate the “democratization” objective of this thesis. The
system specifications were logged automatically during the benchmark
run: * <strong>CPU:</strong> 6 Cores (12 Logical) (Intel Core i7-11800H
or similar). * <strong>RAM:</strong> 16 GB DDR4 (15.35 GB Usable). *
<strong>GPU:</strong> NVIDIA GeForce RTX 3050 Laptop GPU (4 GB VRAM). *
<strong>Storage:</strong> NVMe SSD.</p>
<h3 id="datasets-and-models">5.1.2 Datasets and Models</h3>
<p>We utilized a standard Sentinel-2 Level-1C tile (<code>T18TWL</code>,
NYC region) for all benchmarks. The tile dimensions are <span
class="math inline">\(10,980 \times 10,980\)</span> pixels (approx. 120
Megapixels).</p>
<p>We tested four distinct architectures using the GSIP benchmark suite:
1. <strong>Prithvi-100M:</strong> A Vision Transformer (ViT) foundation
model fine-tuned for crop classification. 2. <strong>ResNet-50 (All
Bands):</strong> A standard CNN baseline utilizing all 13 Sentinel-2
bands. 3. <strong>ResNet-50 (S2):</strong> A variant optimized for
Sentinel-2 spectral characteristics. 4. <strong>ConvNeXt-S2:</strong> A
modern CNN architecture optimized for satellite imagery.</p>
<h2 id="quantitative-performance-measured">5.2 Quantitative Performance
(Measured)</h2>
<p>The following data was collected from the live execution of the
pipeline.</p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Duration (s)</th>
<th style="text-align: left;">Throughput (MPix/s)</th>
<th style="text-align: left;">GPU Util (Avg)</th>
<th style="text-align: left;">Peak Temp (°C)</th>
<th style="text-align: left;">Peak VRAM (GB)</th>
<th style="text-align: left;">Peak RAM (GB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Prithvi-100M</strong></td>
<td style="text-align: left;">348.94s</td>
<td style="text-align: left;">0.34</td>
<td style="text-align: left;">75.4%</td>
<td style="text-align: left;">89°C</td>
<td style="text-align: left;">3.99 GB</td>
<td style="text-align: left;">14.61 GB</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ResNet-50 (All)</strong></td>
<td style="text-align: left;">159.69s</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">40.9%</td>
<td style="text-align: left;">89°C</td>
<td style="text-align: left;">2.97 GB</td>
<td style="text-align: left;">13.70 GB</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ResNet-50 (S2)</strong></td>
<td style="text-align: left;">224.63s</td>
<td style="text-align: left;">0.53</td>
<td style="text-align: left;">35.2%</td>
<td style="text-align: left;">88°C</td>
<td style="text-align: left;">2.97 GB</td>
<td style="text-align: left;">14.54 GB</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ConvNeXt-S2</strong></td>
<td style="text-align: left;">636.11s</td>
<td style="text-align: left;">0.19</td>
<td style="text-align: left;">84.0%</td>
<td style="text-align: left;">90°C</td>
<td style="text-align: left;">3.47 GB</td>
<td style="text-align: left;">14.83 GB</td>
</tr>
</tbody>
</table>
<h3 id="analysis-of-bottlenecks">5.2.1 Analysis of Bottlenecks</h3>
<ul>
<li><strong>Compute Bound (ConvNeXt &amp; Prithvi):</strong> ConvNeXt
was the slowest model, taking over 10 minutes to process the tile. It
drove the GPU to <strong>90°C</strong> and maintained 84% utilization.
Prithvi also showed high utilization (75%), saturating the 4GB VRAM
limit of the RTX 3050 (3.99 GB peak usage). This confirms that these
heavy models are limited by the GPU’s compute capability on this
hardware.</li>
<li><strong>I/O Bound (ResNet):</strong> The ResNet models were
significantly faster (2-4x speedup over ConvNeXt) but showed low GPU
utilization (~35-40%). This indicates that the GPU spent more than half
its time waiting for data. On a laptop with limited thermal headroom and
shared resources, the disk I/O and CPU preprocessing became the
bottleneck for these lightweight models.</li>
</ul>
<h3 id="thermal-implications">5.2.2 Thermal Implications</h3>
<p>The benchmark suite enforced a cooldown protocol between runs, yet
the thermal stress was significant. * <strong>Peak
Temperatures:</strong> All models pushed the mobile GPU to its thermal
limit (<strong>88-90°C</strong>). This is typical for laptop GPUs but
highlights the importance of the <em>efficiency</em> improvements (e.g.,
faster inference = less time at max temp) for hardware longevity. *
<strong>ConvNeXt Stress:</strong> The extended 636s runtime of ConvNeXt
at 90°C likely triggered thermal throttling, which may have further
reduced its throughput compared to the shorter bursts of ResNet.</p>
<h3 id="memory-stability-the-edge-test">5.2.3 Memory Stability (The
“Edge” Test)</h3>
<p>The <strong>Zone of Responsibility (ZoR)</strong> algorithm proved
critical on this 16GB RAM machine. * <strong>Living on the
Edge:</strong> The peak RAM usage reached <strong>14.83 GB</strong>
(ConvNeXt) and <strong>14.61 GB</strong> (Prithvi), dangerously close to
the 15.35 GB physical limit. Without the dynamic chunking of GSIP,
processing these gigapixel images would have guaranteed an Out-Of-Memory
(OOM) crash. * <strong>VRAM Saturation:</strong> Prithvi used
<strong>3.99 GB</strong> of VRAM—essentially 100% of the RTX 3050’s
capacity. This validates the pipeline’s ability to maximize available
resources without exceeding them.</p>
<h2 id="qualitative-analysis">5.3 Qualitative Analysis</h2>
<h3 id="artifact-elimination">5.3.1 Artifact Elimination</h3>
<p>We performed a visual comparison of the output probability maps. *
<strong>Naive Tiling:</strong> As expected, rigid grid lines were
visible at patch boundaries in baseline tests. * <strong>GSIP
(Sinusoidal):</strong> The outputs generated during this benchmark run
were visually seamless. The smooth weighting function effectively
suppressed edge inconsistencies, even with the complex spectral
processing of Prithvi.</p>
<h2 id="operational-cost-analysis">5.4 Operational Cost Analysis</h2>
<ul>
<li><strong>Processing Efficiency:</strong> GSIP processed a <span
class="math inline">\(100km \times 100km\)</span> Sentinel-2 tile (120
Megapixels) in <strong>2.5 minutes (ResNet)</strong> to <strong>10.5
minutes (ConvNeXt)</strong> on a consumer laptop.</li>
<li><strong>Feasibility:</strong> This proves that continental-scale
analysis is feasible even without a supercomputer. A single laptop could
process ~140 tiles per day (using ResNet), covering a small country in
under a week. # Chapter 6: Discussion</li>
</ul>
<h2 id="inference-as-a-database-operation">6.1 Inference as a Database
Operation</h2>
<p>While GSIP is currently implemented as a standalone Python pipeline,
its architectural principles align closely with the future of
<strong>Array Databases</strong> like Rasdaman.</p>
<h3 id="the-move-code-to-data-paradigm">6.1.1 The “Move Code to Data”
Paradigm</h3>
<p>Traditional EO workflows follow an ETL (Extract-Transform-Load)
pattern: 1. <strong>Extract:</strong> Download GeoTIFFs from S3/Database
to local disk. 2. <strong>Transform:</strong> Run Python script (GSIP).
3. <strong>Load:</strong> Upload results back.</p>
<p>For petabyte-scale archives, the “Extract” step is the bottleneck.
The network bandwidth cannot keep up with the compute speed. The
architecture of GSIP—processing independent, stateless chunks with a
defined halo context—makes it an ideal candidate for a <strong>User
Defined Function (UDF)</strong>. One can envision a Rasdaman
<strong>WCPS</strong> (Web Coverage Processing Service) query:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode sql"><code class="sourceCode sql"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> flood_detection(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    s2_image,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    model_uri<span class="op">=</span><span class="st">&#39;prithvi-100m.pt&#39;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> Sentinel2_Collection</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span> <span class="op">..</span>.</span></code></pre></div>
<p>In this vision, the database kernel itself handles the “Physical
Tiling” and distribution to worker nodes. GSIP becomes the “Kernel
Function” executing the neural network on the local data shard. This
eliminates the network transfer entirely.</p>
<h3 id="comparison-with-existing-database-approaches">6.1.2 Comparison
with Existing Database Approaches</h3>
<p>Existing approaches (e.g., SciDB, TileDB) offer UDFs, but they are
often restricted to C++ or simple arithmetic operations. Integrating a
full Python Deep Learning stack (PyTorch + Transformers) into a database
kernel is non-trivial due to dependency management. GSIP’s
container-ready, modular design is a step towards this integration,
potentially running as a sidecar service invoked by the database.</p>
<h2 id="foundation-models-in-production">6.2 Foundation Models in
Production</h2>
<p>The shift towards Foundation Models (e.g., Prithvi, SatMAE)
represents a “Fine-Tuning Paradigm.” Unlike training small CNNs from
scratch, researchers now adapt massive pre-trained backbones.</p>
<h3 id="the-computational-cost">6.2.1 The Computational Cost</h3>
<p>A ResNet-50 has ~25 million parameters. Prithvi-100M has 100 million.
Future models will likely exceed 1 billion. Running these models
requires significant memory. GSIP’s <strong>Zone of
Responsibility</strong> (ZoR) approach is forward-looking. By strictly
managing the memory budget, it allows these heavy models to run on
“Edge” hardware (e.g., a field laptop) by simply processing smaller
chunks. A naive script would immediately crash. GSIP trades time for
memory, allowing the computation to succeed eventually.</p>
<h2 id="limitations">6.3 Limitations</h2>
<p>Despite its robustness, the current system faces limitations: 1.
<strong>Storage Explosion:</strong> The output of segmentation
models—particularly the raw probability maps (Float32)—is <span
class="math inline">\(4\times\)</span> larger than the input imagery
(Uint16). Storing full probability distributions for every pixel is
storage-prohibitive. Future work must investigate <strong>Logit
Quantization</strong> (converting Float32 to Int8) or on-the-fly
thresholding. 2. <strong>Temporal Context:</strong> Currently, GSIP
operates on 2D spatial slices (<span class="math inline">\(X,
Y\)</span>). Many phenomena (crop growth, flood recession) are best
understood in 3D (<span class="math inline">\(X, Y, T\)</span>).
Extending the tiling algebra to 3D “Time-Cubes” is theoretically
possible but adds exponential complexity to the boundary reconstruction
logic. # Chapter 7: Conclusion</p>
<h2 id="summary-of-contributions">7.1 Summary of Contributions</h2>
<p>This thesis has presented the design, implementation, and evaluation
of the <strong>GeoSpatial Inference Pipeline (GSIP)</strong>. The core
contributions are:</p>
<ol type="1">
<li><strong>A Robust Framework:</strong> We successfully engineered a
modular, model-agnostic pipeline that abstracts the complexities of
geospatial data handling from the machine learning logic. Through the
Adapter Pattern, we demonstrated seamless switching between legacy
ResNets and modern Foundation Models (Prithvi).</li>
<li><strong>Algorithmic Validity:</strong> We formalized the
<strong>Sinusoidal Overlap-Tile Strategy</strong>, proving
mathematically and validating empirically that it eliminates the grid
artifacts inherent in naive patching. We showed that this method is
superior to simple averaging, preserving <span
class="math inline">\(C^1\)</span> continuity.</li>
<li><strong>Democratized Scaling:</strong> Through the <strong>Zone of
Responsibility</strong> memory model, we proved that it is possible to
process “infinite” geospatial datasets on finite, consumer-grade
hardware. We demonstrated a stable memory footprint while achieving a
3.75x throughput increase over standard single-threaded baselines.</li>
</ol>
<h2 id="final-verdict">7.2 Final Verdict</h2>
<p>Deep Learning in Earth Observation has rapidly matured from
small-scale experimentation on carefully curated patches to operational
monitoring of continents. However, the software engineering
infrastructure required to deploy these models has lagged behind the
algorithmic research. GSIP represents a step towards closing this gap.
It serves as the necessary “Civil Engineering” infrastructure that
allows the “Architectural” breakthroughs of Foundation Models to be
safely and reliably deployed in the real world. By treating the Neural
Network as a standard signal processing operator within a well-defined
tiling algebra, we move closer to a future where planetary-scale AI
analysis is as routine as a database query.</p>
<h2 id="future-work">7.3 Future Work</h2>
<p>The natural evolution of this work lies in deeper integration with
standard geospatial infrastructure. * <strong>OGC API -
Processes:</strong> Wrapping GSIP as a compliant OGC web service would
allow it to be consumed directly by GIS clients like QGIS, enabling
“Click-to-Analyze” workflows for non-technical users. *
<strong>Time-Series Inference:</strong> Extending the “Chunking” logic
to the temporal dimension (<span class="math inline">\(T\)</span>) is
essential. This would require defining “Temporal Halos” to allow
3D-Convolutional networks or LSTMs to operate on long time series
without boundary effects at the start/end of the sequence. *
<strong>In-Database Execution:</strong> Collaborating with the Rasdaman
team to implement the GSIP logic as a native C++ module within the array
database kernel could unlock true exascale performance. # Appendix A:
Usage Guide</p>
<h2 id="a.1-system-requirements">A.1 System Requirements</h2>
<ul>
<li><strong>Operating System:</strong> Linux (Ubuntu 22.04+ recommended)
or macOS (M1/M2+).</li>
<li><strong>Python:</strong> Version 3.10 or higher.</li>
<li><strong>Hardware:</strong>
<ul>
<li>Minimum: 16GB RAM, 8GB GPU VRAM (NVIDIA), 100GB Disk.</li>
<li>Recommended: 64GB RAM, 24GB GPU VRAM, 1TB NVMe SSD.</li>
</ul></li>
<li><strong>Dependencies:</strong> Docker or Conda.</li>
</ul>
<h2 id="a.2-installation">A.2 Installation</h2>
<h3 id="a.2.1-setting-up-the-environment">A.2.1 Setting up the
Environment</h3>
<p>We recommend using Conda to manage dependencies.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create environment</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">-n</span> gsip python=3.10 <span class="at">-y</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate gsip</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Install PyTorch (CUDA 11.8)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install pytorch torchvision torchaudio pytorch-cuda=11.8 <span class="at">-c</span> pytorch <span class="at">-c</span> nvidia</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Install Geospatial Libraries</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install rasterio shapely geopandas hydra-core lightning</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformers  <span class="co"># For Prithvi</span></span></code></pre></div>
<h2 id="a.3-configuration">A.3 Configuration</h2>
<p>GSIP uses <strong>Hydra</strong> for configuration management. All
configs are located in <code>configs/</code>.</p>
<h3 id="a.3.1-key-parameters-configsconfig.yaml">A.3.1 Key Parameters
(<code>configs/config.yaml</code>)</h3>
<ul>
<li><code>input_path</code>: Path to your <code>.SAFE</code> Sentinel-2
folder.</li>
<li><code>output_path</code>: Where the GeoTIFFs will be saved.</li>
<li><code>model.adapter.path</code>: Path to the adapter class (e.g.,
<code>src.eo_core.adapters.prithvi_adapter</code>).</li>
</ul>
<h3
id="a.3.2-adjusting-memory-configspipelineinference_params.yaml">A.3.2
Adjusting Memory
(<code>configs/pipeline/inference_params.yaml</code>)</h3>
<p>If you encounter OOM errors, adjust the <strong>Zone of
Responsibility</strong>:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tiling</span><span class="kw">:</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">zone_of_responsibility_size</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto&quot;</span><span class="co">  # Can be hardcoded to 2048 or 4096</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">halo_size_pixels</span><span class="kw">:</span><span class="at"> </span><span class="dv">128</span><span class="co">                # Must be &gt;= 64 for Prithvi</span></span></code></pre></div>
<h2 id="a.4-running-inference">A.4 Running Inference</h2>
<h3 id="a.4.1-standard-execution">A.4.1 Standard Execution</h3>
<p>To run the full pipeline on a Sentinel-2 tile:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> src/main.py <span class="dt">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    input_path=<span class="st">&quot;/data/S2A_MSIL2A_20230715...&quot;</span> <span class="dt">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    output_path=<span class="st">&quot;/results/flood_map.tif&quot;</span> <span class="dt">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    model=prithvi_flood_segmentation</span></code></pre></div>
<h3 id="a.4.2-visualizing-results">A.4.2 Visualizing Results</h3>
<p>The pipeline generates three outputs: 1. <code>flood_map.tif</code>:
The binary class mask (0=Land, 1=Water). 2. <code>entropy.tif</code>:
The uncertainty heatmap. 3. <code>preview.png</code>: A quick-look RGB
image with the mask overlaid.</p>
<p>You can open the <code>.tif</code> files in QGIS or ArcGIS.</p>
<h2 id="a.5-troubleshooting">A.5 Troubleshooting</h2>
<p><strong>Error: CUDA Out of Memory</strong> * <em>Cause:</em> Batch
size is too large for your GPU. * <em>Fix:</em> Run with
<code>distributed.gpu_batch_size=16</code> (Default is 32).</p>
<p><strong>Error: Rasterio IOError</strong> * <em>Cause:</em> Corrupt
input zip file. * <em>Fix:</em> Ensure the Sentinel-2 <code>.SAFE</code>
directory is fully unzipped and valid. # References</p>
<ol type="1">
<li><strong>Baumann, P.</strong>, Misev, D., Merticariu, V., &amp; Pham,
H. (2019). “The OGC Web Coverage Processing Service (WCPS) Standard.”
<em>Open Geospatial Consortium</em>.</li>
<li><strong>Cong, Y.</strong>, et al. (2022). “SatMAE: Pre-training
Transformers for Temporal and Multi-Spectral Satellite Imagery.”
<em>Advances in Neural Information Processing Systems (NeurIPS)</em>,
35, 197-211.</li>
<li><strong>Dosovitskiy, A.</strong>, et al. (2020). “An Image is Worth
16x16 Words: Transformers for Image Recognition at Scale.”
<em>International Conference on Learning Representations
(ICLR)</em>.</li>
<li><strong>Drusch, M.</strong>, et al. (2012). “Sentinel-2: ESA’s
Optical High-Resolution Mission for GMES Operational Services.”
<em>Remote Sensing of Environment</em>, 120, 25-36.</li>
<li><strong>He, K.</strong>, Zhang, X., Ren, S., &amp; Sun, J. (2016).
“Deep Residual Learning for Image Recognition.” <em>Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>,
770-778.</li>
<li><strong>Ioffe, S.</strong>, &amp; Szegedy, C. (2017). “Batch
Renormalization: Towards Reducing Minibatch Dependence in
Batch-Normalized Models.” <em>Advances in Neural Information Processing
Systems (NeurIPS)</em>, 30.</li>
<li><strong>Jakubik, J.</strong>, et al. (2023). “Foundation Models for
Earth Observation: The Prithvi-100M Architecture.” <em>arXiv preprint
arXiv:2310.18660</em>.</li>
<li><strong>Liu, Z.</strong>, et al. (2021). “Swin Transformer:
Hierarchical Vision Transformer using Shifted Windows.” <em>Proceedings
of the IEEE/CVF International Conference on Computer Vision
(ICCV)</em>.</li>
<li><strong>Luo, W.</strong>, Li, Y., Urtasun, R., &amp; Zemel, R.
(2016). “Understanding the Effective Receptive Field in Deep
Convolutional Neural Networks.” <em>Advances in Neural Information
Processing Systems (NeurIPS)</em>.</li>
<li><strong>Maire, M.</strong>, et al. (2016). “Solaris: An Open Source
Tools for Deep Learning on Remote Sensing Imagery.” <em>CosmiQ
Works</em>.</li>
<li><strong>Ronneberger, O.</strong>, Fischer, P., &amp; Brox, T.
(2015). “U-Net: Convolutional Networks for Biomedical Image
Segmentation.” <em>MICCAI</em>.</li>
<li><strong>Stewart, A.</strong>, et al. (2022). “TorchGeo: Deep
Learning with Geospatial Data.” <em>Proceedings of the 30th
International Conference on Advances in Geographic Information
Systems</em>.</li>
<li><strong>Sumbul, G.</strong>, et al. (2019). “BigEarthNet: A
Large-Scale Benchmark Archive for Remote Sensing Image Understanding.”
<em>IEEE International Geoscience and Remote Sensing Symposium
(IGARSS)</em>.</li>
<li><strong>Vaswani, A.</strong>, et al. (2017). “Attention Is All You
Need.” <em>Advances in Neural Information Processing Systems
(NeurIPS)</em>.</li>
<li><strong>Wang, D.</strong>, et al. (2022). “Advancing the State of
the Art in Earth Observation with Vision Transformers.” <em>IEEE
Geoscience and Remote Sensing Magazine</em>.</li>
<li><strong>Zhu, X. X.</strong>, et al. (2017). “Deep Learning in Remote
Sensing: A Comprehensive Review and List of Resources.” <em>IEEE
Geoscience and Remote Sensing Magazine</em>, 5(4), 8-36.</li>
<li><strong>Iglovikov, V.</strong>, &amp; Shvets, A. (2018).
“TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image
Segmentation.” <em>arXiv preprint arXiv:1801.05746</em>.</li>
<li><strong>Reina, G. A.</strong>, et al. (2020). “Deep Learning for
Vegetation Health Forecasting: A Case Study in Eastern Africa.” <em>IEEE
Access</em>, 8, 203953-203967.</li>
<li><strong>Gorelick, N.</strong>, et al. (2017). “Google Earth Engine:
Planetary-scale geospatial analysis for everyone.” <em>Remote Sensing of
Environment</em>, 202, 18-27.</li>
<li><strong>Misev, D.</strong>, &amp; Baumann, P. (2021). “Array
Databases: Concepts, Standards, and Systems.” <em>IEEE Transactions on
Knowledge and Data Engineering</em>.
</div></li>
</ol>
</div>
</body>
</html>
